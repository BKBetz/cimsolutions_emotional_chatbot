{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Endproduct\n",
        "\n",
        "In this notebook. Instead of using the FAST API, a notebook will be used to create the proof of concept. This decision was made due to the lack of time for this project. This notebook will combine all the seperate products that have been researsched during this project.\n",
        "\n",
        "The products:\n",
        "* The emotion detection model\n",
        "* The translators\n",
        "* The pre-trained chatbot\n",
        "* The retrieval based output with the feedback algorithm\n",
        "\n",
        "For each product, a Class will be made to make objects and calls.\n",
        "\n",
        "The emotion detection model was saved in the google drive of the creator. Note that in order to make it work, u need to change the path to the path were the model is saved.\n",
        "\n",
        "Note: This notebook is made in google colab. All file paths are to files in the google drive. If used elsewhere, change necessary paths to correct paths in order to connect everything."
      ],
      "metadata": {
        "id": "Fp8tKcOpsiRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary libraries\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sklearn\n",
        "!pip install pandas\n",
        "!pip install sentencepiece\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCD_rY6ntzWu",
        "outputId": "3698069e-ab38-4dd3-b80e-965bc4df5d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post10.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post10-py3-none-any.whl size=2959 sha256=2857bbdfdf4bfeb847b551b43c20406fe651f452fb8a7588b70f19a5d9035b84\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/f6/92/0173054cc528db7ffe7b0c7652a96c3102aab156a6da960387\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post10\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "from transformers import AutoTokenizer, BlenderbotSmallForConditionalGeneration, AutoModelForSeq2SeqLM\n",
        "from transformers import RobertaTokenizer\n",
        "import tqdm\n",
        "import torch\n",
        "from torch import cuda\n",
        "import sqlite3 as sql\n",
        "from google.colab import drive\n",
        "import random as rd\n",
        "import sentencepiece\n",
        "import os\n",
        "import re\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "RxVPeGmduK5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVpiAfiWwDBA",
        "outputId": "f55154db-48b5-4f3c-ad0d-35a611f9c19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect cuda to gpu\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "vRNIzR0Ww8NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Emotion detecion model\n",
        "\n",
        "Retrieve the emotion model to make predictions."
      ],
      "metadata": {
        "id": "5JgZYrZCtq3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve robbert tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\", truncation=True, do_lower_case=True)"
      ],
      "metadata": {
        "id": "N-Fjo5WHzzJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve model using path\n",
        "path = \"drive/MyDrive/HBO/cursussen/jaar_4/afstuderen/robbert_model.pth\"\n",
        "model = torch.load(path)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z91hhabhteJV",
        "outputId": "03786942-8ff2-4a43-f1a1-044fcc02d4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(40000, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=7, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo8Lo26aIgja"
      },
      "outputs": [],
      "source": [
        "\"\"\" This class contains the robbert model. which detects an emotion. emotions are labeled and send back to the user.\"\"\"\n",
        "class RobbertModel:\n",
        "  def __init__(self, model, tokenizer, device) -> None:\n",
        "    self.model =  model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.labels = {0: 'neutral', 1:'joy', 2:'fear', 3:'anger', 4:'sad', 5:'love', 6:\"other\"}\n",
        "    self.device = device\n",
        "\n",
        "  def convert(self, text: str):\n",
        "    # tokenize text to make predictions\n",
        "    tokenized = self.tokenizer(text)\n",
        "\n",
        "    tt = {\n",
        "    'ids': torch.tensor(tokenized['input_ids'], dtype=torch.long),\n",
        "    'mask': torch.tensor(tokenized['attention_mask'], dtype=torch.long),\n",
        "    }\n",
        "\n",
        "    ids = tt['ids'].to(self.device, dtype = torch.long)\n",
        "    mask = tt['mask'].to(self.device, dtype = torch.long)\n",
        "    return ids, mask\n",
        "\n",
        "  def predict(self, text: str) -> str:\n",
        "    ids, mask = self.convert(text)\n",
        "    output = self.model(ids.unsqueeze(0), mask.unsqueeze(0))\n",
        "    emotion =  self.labels[output.logits.argmax(1).item()]\n",
        "\n",
        "    return emotion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "robbert = RobbertModel(model, tokenizer, device)"
      ],
      "metadata": {
        "id": "yXe0fD40EhQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "robbert.predict(\"ik ben blij\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dsHjhXrrEws0",
        "outputId": "9445a4ae-afd3-4fd4-ef8e-4bd97b4c25ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'love'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translators\n",
        "\n",
        "Create a translation class that contains both the english and dutch translators to make easy calls using one class"
      ],
      "metadata": {
        "id": "41wQBSUcJYZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import models and tokenizers\n",
        "tokenizer_en = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-nl-en\")\n",
        "tokenizer_nl = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2rodBg3JuZH",
        "outputId": "329bcf99-d353-4a1e-cbbe-c55047fe6407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_en = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-nl-en\")"
      ],
      "metadata": {
        "id": "EjKoXC3qJ26d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_nl = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\")"
      ],
      "metadata": {
        "id": "LXoEgNDJJ8AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This class contains both translators. Based on the given language input,\n",
        "the sentence gets translated from en-nl or nl-en\"\"\"\n",
        "\n",
        "class Translators:\n",
        "  def __init__(self, en_tokenizer, nl_tokenizer, en_model, nl_model) -> None:\n",
        "    self.en_tokenizer = en_tokenizer\n",
        "    self.nl_tokenizer = nl_tokenizer\n",
        "    self.en_model = en_model\n",
        "    self.nl_model = nl_model\n",
        "\n",
        "  def en_translate(self, text: str) -> str:\n",
        "      # translate to english\n",
        "      batch = self.en_tokenizer([text], return_tensors=\"pt\")\n",
        "      generated_ids = self.en_model.generate(**batch)\n",
        "      translation = self.en_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "      return translation\n",
        "\n",
        "  def nl_translate(self, text: str) -> str:\n",
        "      # translate to dutch\n",
        "      batch = self.nl_tokenizer([text], return_tensors=\"pt\")\n",
        "      generated_ids = self.nl_model.generate(**batch)\n",
        "      translation = self.nl_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "      return translation\n",
        "\n",
        "  def translate(self, text: str, language:str) -> str:\n",
        "    # choose translator based on given language\n",
        "    if language == \"nl\":\n",
        "      translated = self.nl_translate(text)\n",
        "    elif language == \"en\":\n",
        "      translated = self.en_translate(text)\n",
        "    else:\n",
        "      translated = \"Language not found\"\n",
        "\n",
        "    return translated\n"
      ],
      "metadata": {
        "id": "IXEpwFyyEzmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translators(tokenizer_en, tokenizer_nl, model_en, model_nl)"
      ],
      "metadata": {
        "id": "ltGyiWF2KwlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "nl_en = translator.translate(\"ik ben blij\", \"en\")\n",
        "en_nl = translator.translate(\"I'm happy\", \"nl\")\n",
        "\n",
        "print(nl_en, en_nl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6LBRZXKK5QO",
        "outputId": "c4281ddc-68e1-410d-f9c0-eb5b4b2e3ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm glad Ik ben gelukkig.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-trained Chatbot\n",
        "Importing the 90M small blenderbot from facebook. This chatbot will be used for simple conversation. The class will remove any unnecessary characters."
      ],
      "metadata": {
        "id": "jEE4N2dmMJpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import model and tokenizer\n",
        "blenderbot = BlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\", add_cross_attention=False)\n",
        "bb_tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")"
      ],
      "metadata": {
        "id": "EOEdNR9ULCa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This class contains the pre trained blenderbot. Using it's tokenizer, it returns a given input.\"\"\"\n",
        "\n",
        "class Chatbot:\n",
        "  def __init__(self, model, tokenizer) -> None:\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def convert(self, text: str):\n",
        "    # convert to reply_ids which in return provide a output from the chatbot\n",
        "    inputs = self.tokenizer([text], return_tensors=\"pt\")\n",
        "    reply_ids = self.model.generate(**inputs)\n",
        "    return reply_ids\n",
        "\n",
        "  def clean_string(self, text: str) -> str:\n",
        "    # using split. the sentence will always be in the same place. This way u can easily retrieve it\n",
        "\n",
        "    splitted_text = text.split('_')\n",
        "    text = splitted_text[4]\n",
        "\n",
        "    return text\n",
        "\n",
        "  def response(self, text: str) -> str:\n",
        "    # generate clean response from chatbot\n",
        "    reply_ids = self.convert(text)\n",
        "    output = self.tokenizer.batch_decode(reply_ids)\n",
        "    response = self.clean_string(output[0])\n",
        "    return response"
      ],
      "metadata": {
        "id": "O8s7ZIpDMh-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = Chatbot(blenderbot, bb_tokenizer)"
      ],
      "metadata": {
        "id": "6qMdOqMiN0jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "res = chatbot.response(\"how are you doing\")\n",
        "print(res)"
      ],
      "metadata": {
        "id": "HipZnw_ON4VO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a02dfe-4eb7-4779-e5b6-2691cfea5987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " i'm doing well, thank you. what about you? what are you up to? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval based output and database\n",
        "Create the retrieval based output class and connect with the database. This simulates the API connection by making the retrieval based system work together with the robbert model."
      ],
      "metadata": {
        "id": "kh-sxSHGsBXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect with database"
      ],
      "metadata": {
        "id": "yZXLnMW6scHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_path = \"drive/MyDrive/HBO/cursussen/jaar_4/afstuderen/ed_outputs.db\""
      ],
      "metadata": {
        "id": "kTI1gPkMN8Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make connection to the database\n",
        "conn = sql.connect(db_path)\n",
        "cur = conn.cursor()"
      ],
      "metadata": {
        "id": "OzB10yQjsbCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# database content\n",
        "cur.execute(\"SELECT id, output, emotion, score FROM outputs\")\n",
        "cur.fetchall()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCkORBpCsmq2",
        "outputId": "2cd43380-1df1-45af-a252-d3a8429f6331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'Laten we de situatie bespreken en een oplossing vinden.', 'angry', 0),\n",
              " (2,\n",
              "  ' Ik begrijp dat je het niet leuk vindt. Ik ge voor jou een oplossing zoeken.',\n",
              "  'angry',\n",
              "  0),\n",
              " (3,\n",
              "  'Laten we kijken wat er aan de hand is en bekijken hoe we het kunnen oplossen.',\n",
              "  'angry',\n",
              "  0),\n",
              " (4,\n",
              "  'Laten we kalm blijven en samen naar een bruikbare oplossing zoeken.',\n",
              "  'angry',\n",
              "  0),\n",
              " (5,\n",
              "  'Als er zaken zijn die moeten worden opgelost, laten we ze uitzoeken  en aanpakken.',\n",
              "  'angry',\n",
              "  0),\n",
              " (6, 'Fijn om te horen dat je tevreden bent!', 'joy', 0),\n",
              " (7, 'Het is goed om te horen dat je zo blij bent!', 'joy', 0),\n",
              " (8, 'Dat is goed nieuws, fijn!', 'joy', 0),\n",
              " (9, 'Graag gedaan, blij dat ik kan helpen!', 'joy', 0),\n",
              " (10, 'Wat leuk!', 'joy', 0),\n",
              " (11, 'Laat me weten hoe ik kan helpen.', 'sad', 0),\n",
              " (12, 'Als je iets nodig hebt, aarzel dan niet om te vragen.', 'sad', 0),\n",
              " (13, 'We gaan samen een oplossing zoeken..', 'sad', 0),\n",
              " (14, 'Als je hulp nodig hebt, laat het me weten.', 'sad', 0),\n",
              " (15, 'Als er iets is dat je wilt bespreken, sta ik voor je klaar.', 'sad', 0),\n",
              " (16, 'Kan ik iets doen om je gerust te stellen?', 'fear', 0),\n",
              " (17, 'Kan ik iets doen tegen je angst?', 'fear', 0),\n",
              " (18, 'Wees niet Bang, we gaan het oplosssen', 'fear', 0),\n",
              " (19, 'Is er iets dat je zorgen kan wegnement?', 'fear', 0),\n",
              " (20, 'Kan iets helpen bij het verminderen van je angst?', 'fear', 0),\n",
              " (21, 'Ik waardeer je houding.', 'love', 0),\n",
              " (22, 'Jouw medewerking is fijn.', 'love', 0),\n",
              " (23, 'Je positieve houding draagt bij aan een prettig gesprek.', 'love', 0),\n",
              " (24, 'Bedankt voor je inzet en toewijding.', 'love', 0),\n",
              " (25, 'Je bijdrage wordt gewaardeerd.', 'love', 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This class works with the robbert model class.\n",
        "To provide a response given a detected emotion.\n",
        "The response comes from the database\"\"\"\n",
        "\n",
        "class RetrievalOutput:\n",
        "  def __init__(self, conn, cur, epsilon, ed_model):\n",
        "    self.database = conn,\n",
        "    self.cur = cur\n",
        "    self.memory = []\n",
        "    self.ed_model = ed_model\n",
        "    self.conv_id = 0\n",
        "    self.epsilon = epsilon\n",
        "    self. categories = {\"pos\": [\"joy\", \"love\"], \"neu\": [\"neutral\", \"other\"], \"neg\": [\"sad\", \"angry\", \"fear\"]}\n",
        "    self.grading = {\"neg-neg\": -1, \"neg-neu\": 1, \"neg-pos\": 2, \"neu-neu\": 0, \"neu-pos\": 1, \"neu-neg\": -1, \"pos-pos\": 1, \"pos-neu\": 0, \"pos-neg\": -2}\n",
        "\n",
        "  def check_emotion(self, text: str) -> str:\n",
        "    # in this function get the emotion of a text\n",
        "    emotion = self.ed_model.predict(text)\n",
        "    # small print statement to show in the test conversation\n",
        "    print(\"detected_emotion: \", emotion)\n",
        "    return emotion\n",
        "\n",
        "  def get_output(self, emotion:str):\n",
        "    # retrieve from db using policy greedy\n",
        "    if emotion != \"neutral\" or emotion != \"other\":\n",
        "      rd_int = round(rd.random(), 2)\n",
        "      if rd_int > self.epsilon:\n",
        "        # random\n",
        "        query = \"SELECT id, output, emotion, score FROM outputs WHERE emotion = '%s' ORDER BY RANDOM() LIMIT 1\" % emotion\n",
        "      else:\n",
        "        # not random\n",
        "        query = \"SELECT id, output, emotion, score FROM outputs WHERE emotion = '%s' ORDER BY score DESC LIMIT 1\" % emotion\n",
        "\n",
        "      output = self.cur.execute(query)\n",
        "      output = list(self.cur.fetchone())\n",
        "\n",
        "    else:\n",
        "      output = None\n",
        "\n",
        "    return output\n",
        "\n",
        "  def save(self, input:str, rb_output:List, emotion:str):\n",
        "    # save conversation as dict for grading\n",
        "\n",
        "    # get correct category using emotion\n",
        "    for key, value in self.categories.items():\n",
        "      for x in value:\n",
        "        if x == emotion:\n",
        "          category = key\n",
        "    conversation = {\"conv_id\": self.conv_id, \"input\": input, \"rb\": rb_output, \"pnn\": category}\n",
        "    self.memory.append(conversation)\n",
        "\n",
        "  def grade(self):\n",
        "    # grade previous response using current conversation\n",
        "     sentiment = self.memory[self.conv_id]['pnn']\n",
        "     if len(self.memory) > 1:\n",
        "      prev_conv = self.memory[self.conv_id - 1]\n",
        "      prev_sentiment = prev_conv['pnn']\n",
        "\n",
        "      change = prev_sentiment + \"-\" + sentiment\n",
        "      print(\"conversation-change: \", change)\n",
        "      grade = self.grading[change]\n",
        "      # grade the score of the previous response in the database\n",
        "      if prev_sentiment != \"neu\":\n",
        "        prev_conv['rb'][3] = prev_conv['rb'][3] + grade\n",
        "        self.update_db(prev_conv['rb'])\n",
        "\n",
        "     self.conv_id += 1\n",
        "\n",
        "  def update_db(self, response:List):\n",
        "    # update the correct db output with the new score\n",
        "    query = \"UPDATE outputs SET score = %s WHERE id = %s\" % (response[3], response[0])\n",
        "    cur.execute(query)\n",
        "\n",
        "  def reset_score(self):\n",
        "    # reset all scores of db\n",
        "    query = \"UPDATE outputs SET score = 0\"\n",
        "    cur.execute(query)\n",
        "\n",
        "  def reset_memory(self):\n",
        "    # reset memory of class\n",
        "    self.memory.clear()\n",
        "\n",
        "  def forward(self, text:str) -> str:\n",
        "    # return response.\n",
        "    emotion =  self.check_emotion(text)\n",
        "    rb_output =  self.get_output(emotion)\n",
        "    self.save(text, rb_output, emotion)\n",
        "    self.grade()\n",
        "\n",
        "    if rb_output == None:\n",
        "      return rb_output\n",
        "\n",
        "    else:\n",
        "      return rb_output[1]"
      ],
      "metadata": {
        "id": "wgwLAvJ6sq9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rbo =  RetrievalOutput(conn, cur, 0.9, robbert)"
      ],
      "metadata": {
        "id": "Y9rx-2uBvut9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "rbo.forward(\"ik ben bang\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3kcCkcwlOT8i",
        "outputId": "680936af-2692-4fb6-df5c-d4a02a8925f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detected_emotion:  fear\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Kan ik iets doen tegen je angst?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Endproduct\n",
        "Using al the classes, a final class called output will be created. This class uses all other classes to provide an answer to users. Normally this would be the main.py in a normal project."
      ],
      "metadata": {
        "id": "c0xFJ-D-woDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This class creates a conversation agent that can be used to creat small conversations where an emotion is being detected.\n",
        "It combines the chatbot output with the retrieval based output (could be made cleaner with nlp)\"\"\"\n",
        "\n",
        "class ConversationAgent:\n",
        "\n",
        "  def __init__(self, chatbot, translators, rbo):\n",
        "    self.cb = chatbot\n",
        "    self.translators =  translators\n",
        "    self.rbo = rbo\n",
        "\n",
        "  def get_rbo(self, text:str) -> str:\n",
        "    # get output from retrieval based system\n",
        "    rb_output = self.rbo.forward(text)\n",
        "    return rb_output\n",
        "\n",
        "  def cb_response(self, text:str) -> str:\n",
        "    # get output from chatbot\n",
        "    en_text = self.translators.translate(text, \"en\")\n",
        "    cb_text = self.cb.response(en_text)\n",
        "    nl_text =  self.translators.translate(cb_text, \"nl\")\n",
        "    return nl_text\n",
        "\n",
        "  def combine_outputs(self, cb_output:str, retrieval_output:str) -> str:\n",
        "    # combine the two outputs\n",
        "    combined =  cb_output + retrieval_output\n",
        "    return combined\n",
        "\n",
        "  def forward(self, text: str) -> str:\n",
        "    # get response\n",
        "    cb_output =  self.cb_response(text)\n",
        "    retrieval_output = self.get_rbo(text)\n",
        "    combined = self.combine_outputs(cb_output, retrieval_output)\n",
        "    if retrieval_output ==  None:\n",
        "      return cb_output\n",
        "    else:\n",
        "      return combined"
      ],
      "metadata": {
        "id": "UlKdF9tGwZnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "#reset memory and database before the testing starts\n",
        "rbo.reset_score()\n",
        "rbo.reset_memory\n",
        "\n",
        "conv_agent = ConversationAgent(chatbot, translator, rbo)"
      ],
      "metadata": {
        "id": "e8cwfz4JyjBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# small example. don't have to run this cell\n",
        "conv_agent.forward(\"hoe gaat het\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "koL_uL0AzE0B",
        "outputId": "84fbde3c-da05-4731-a780-b3b8ff356cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "detected_emotion:  sad\n",
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ik weet niet of ik terug naar school wil.Als er iets is dat je wilt bespreken, sta ik voor je klaar.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test conversation\n",
        "The cell below lets u have a conversation using the two outputs of the conv_agent class. Run the database cell after to show the scores of all the outputs in the database."
      ],
      "metadata": {
        "id": "MV4QMze7yiKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 0\n",
        "while x < 10:\n",
        "  user_input = input(\"enter your text here: \")\n",
        "  if user_input == \"exit\":\n",
        "    break\n",
        "  else:\n",
        "    response = conv_agent.forward(user_input)\n",
        "    print(\"response: \", response)\n",
        "    x += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du6PAa0K8JxT",
        "outputId": "9bef78c6-c698-4ca4-a6e6-158810ca04d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter your text here: hallo hoe gaat het\n",
            "detected_emotion:  sad\n",
            "conversation-change:  neg-neg\n",
            "response:  Ik ben net terug van een lange dag op het werk.Laat me weten hoe ik kan helpen.\n",
            "enter your text here: ik ben bang\n",
            "detected_emotion:  fear\n",
            "conversation-change:  neg-neg\n",
            "response:  Wat is er aan de hand in je leven waar je bang voor bent?Kan ik iets doen om je gerust te stellen?\n",
            "enter your text here: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cur.execute(\"SELECT id, output, emotion, score FROM outputs\")\n",
        "cur.fetchall()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKyZeZTv8jQX",
        "outputId": "88d50407-c46a-407f-fb81-9ed91f8d427b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'Laten we de situatie bespreken en een oplossing vinden.', 'angry', 0),\n",
              " (2,\n",
              "  ' Ik begrijp dat je het niet leuk vindt. Ik ge voor jou een oplossing zoeken.',\n",
              "  'angry',\n",
              "  0),\n",
              " (3,\n",
              "  'Laten we kijken wat er aan de hand is en bekijken hoe we het kunnen oplossen.',\n",
              "  'angry',\n",
              "  0),\n",
              " (4,\n",
              "  'Laten we kalm blijven en samen naar een bruikbare oplossing zoeken.',\n",
              "  'angry',\n",
              "  0),\n",
              " (5,\n",
              "  'Als er zaken zijn die moeten worden opgelost, laten we ze uitzoeken  en aanpakken.',\n",
              "  'angry',\n",
              "  0),\n",
              " (6, 'Fijn om te horen dat je tevreden bent!', 'joy', 0),\n",
              " (7, 'Het is goed om te horen dat je zo blij bent!', 'joy', 0),\n",
              " (8, 'Dat is goed nieuws, fijn!', 'joy', 0),\n",
              " (9, 'Graag gedaan, blij dat ik kan helpen!', 'joy', 0),\n",
              " (10, 'Wat leuk!', 'joy', 0),\n",
              " (11, 'Laat me weten hoe ik kan helpen.', 'sad', -1),\n",
              " (12, 'Als je iets nodig hebt, aarzel dan niet om te vragen.', 'sad', 0),\n",
              " (13, 'We gaan samen een oplossing zoeken..', 'sad', 0),\n",
              " (14, 'Als je hulp nodig hebt, laat het me weten.', 'sad', 0),\n",
              " (15, 'Als er iets is dat je wilt bespreken, sta ik voor je klaar.', 'sad', 0),\n",
              " (16, 'Kan ik iets doen om je gerust te stellen?', 'fear', 0),\n",
              " (17, 'Kan ik iets doen tegen je angst?', 'fear', -1),\n",
              " (18, 'Wees niet Bang, we gaan het oplosssen', 'fear', 0),\n",
              " (19, 'Is er iets dat je zorgen kan wegnement?', 'fear', 0),\n",
              " (20, 'Kan iets helpen bij het verminderen van je angst?', 'fear', 0),\n",
              " (21, 'Ik waardeer je houding.', 'love', 0),\n",
              " (22, 'Jouw medewerking is fijn.', 'love', 0),\n",
              " (23, 'Je positieve houding draagt bij aan een prettig gesprek.', 'love', 0),\n",
              " (24, 'Bedankt voor je inzet en toewijding.', 'love', 0),\n",
              " (25, 'Je bijdrage wordt gewaardeerd.', 'love', 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C3qUKFtgTFtf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}